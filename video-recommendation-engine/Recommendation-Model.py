# -*- coding: utf-8 -*-
"""GNN_Recommender_Final (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q2s-ZBpymXvsiUnz9uACOGh6hdnCuYqI
"""

import torch
print(torch.__version__)
print(torch.version.cuda)

!pip install torch_geometric

# -*- coding: utf-8 -*-
"""
Social Network Analysis Code with Feature Normalization
"""

import requests
import pandas as pd
import numpy as np
from tqdm import tqdm
import torch
from torch_geometric.data import HeteroData
from sklearn.metrics.pairwise import cosine_similarity
import os
from datetime import datetime

# --- API Setup ---
FLIC_TOKEN = "flic_11d3da28e403d182c36a3530453e290add87d0b4a40ee50f17611f180d47956f"
HEADERS = {"Flic-Token": FLIC_TOKEN}

API_ENDPOINTS = {
    "viewed": "https://api.socialverseapp.com/posts/view?page={page}&page_size=1000&resonance_algorithm=resonance_algorithm_cjsvervb7dbhss8bdrj89s44jfjdbsjd0xnjkbvuire8zcjwerui3njfbvsujc5if",
    "liked": "https://api.socialverseapp.com/posts/like?page={page}&page_size=1000&resonance_algorithm=resonance_algorithm_cjsvervb7dbhss8bdrj89s44jfjdbsjd0xnjkbvuire8zcjwerui3njfbvsujc5if",
    "inspired": "https://api.socialverseapp.com/posts/inspire?page={page}&page_size=1000&resonance_algorithm=resonance_algorithm_cjsvervb7dbhss8bdrj89s44jfjdbsjd0xnjkbvuire8zcjwerui3njfbvsujc5if",
    "rated": "https://api.socialverseapp.com/posts/rating?page={page}&page_size=1000&resonance_algorithm=resonance_algorithm_cjsvervb7dbhss8bdrj89s44jfjdbsjd0xnjkbvuire8zcjwerui3njfbvsujc5if",
    "posts": "https://api.socialverseapp.com/posts/summary/get?page={page}&page_size=1000",
    "users": "https://api.socialverseapp.com/users/get_all?page={page}&page_size=1000"
}

def fetch_all_paginated(endpoint_name, result_key):
    results = []
    page = 1
    while True:
        url = API_ENDPOINTS[endpoint_name].format(page=page)
        resp = requests.get(url, headers=HEADERS)
        resp.raise_for_status()
        data = resp.json()
        items = data.get(result_key, [])
        if not items:
            break
        results.extend(items)
        if len(items) < 1000:
            break
        page += 1
    return results

# --- Fetch Data ---
print("Fetching data...")
df_viewed = pd.DataFrame(fetch_all_paginated("viewed", "posts"))
df_liked = pd.DataFrame(fetch_all_paginated("liked", "posts"))
df_inspired = pd.DataFrame(fetch_all_paginated("inspired", "posts"))
df_rated = pd.DataFrame(fetch_all_paginated("rated", "posts"))
df_posts = pd.DataFrame(fetch_all_paginated("posts", "posts"))
df_users = pd.DataFrame(fetch_all_paginated("users", "users"))

# --- Standardize Timestamp Columns ---
df_viewed = df_viewed.rename(columns={"viewed_at": "timestamp"})
df_liked = df_liked.rename(columns={"liked_at": "timestamp"})
df_inspired = df_inspired.rename(columns={"inspired_at": "timestamp"})
df_rated = df_rated.rename(columns={"rated_at": "timestamp"})

# --- Deduplicate ---
def deduplicate(df):
    return df.drop_duplicates(subset=["user_id", "post_id"])

df_viewed = deduplicate(df_viewed)
df_liked = deduplicate(df_liked)
df_inspired = deduplicate(df_inspired)
df_rated = deduplicate(df_rated)

# --- Assign Interaction Strength and Build Edge List ---
INTERACTION_WEIGHTS = {
    "viewed": 1,
    "liked": 3,
    "inspired": 4,
    "rated": None  # Use actual rating value (from rating_percent)
}

def interaction_edges(df, interaction_type, weight=None):
    df = df.copy()
    if interaction_type == "rated":
        df = df[df["rating_percent"].notnull()]
        df["weight"] = df["rating_percent"].astype(float) / 100.0  # Scale 0–100 to 0–1
    else:
        df["weight"] = weight
    df["interaction_type"] = interaction_type
    return df[["user_id", "post_id", "weight", "interaction_type", "timestamp"]]

print("Building edge list...")
edges = pd.concat([
    interaction_edges(df_viewed, "viewed", INTERACTION_WEIGHTS["viewed"]),
    interaction_edges(df_liked, "liked", INTERACTION_WEIGHTS["liked"]),
    interaction_edges(df_inspired, "inspired", INTERACTION_WEIGHTS["inspired"]),
    interaction_edges(df_rated, "rated")
], ignore_index=True)

# --- Normalize Timestamps and Compute Recency/Strength ---
edges["timestamp"] = pd.to_datetime(edges["timestamp"])
max_time = edges["timestamp"].max()
min_time = edges["timestamp"].min()
edges["recency"] = (edges["timestamp"] - min_time) / (max_time - min_time + pd.Timedelta(seconds=1))
edges["recency"] = edges["recency"].astype(float)
edges["strength"] = edges["weight"] * (1 + edges["recency"])

# --- Prepare Node Features ---
# User features
user_features = df_users.set_index("id")[["post_count", "following_count", "follower_count"]].fillna(0)

# Post features
def extract_category_id(category):
    if isinstance(category, dict):
        return category.get("id", 0)
    return 0

df_posts["category_id"] = df_posts["category"].apply(extract_category_id)
post_features = df_posts.set_index("id")[["category_id", "view_count", "upvote_count", "average_rating"]].fillna(0)

# --- Encode Node IDs ---
user_id_map = {uid: idx for idx, uid in enumerate(user_features.index)}
post_id_map = {pid: idx for idx, pid in enumerate(post_features.index)}
edges = edges[edges["user_id"].isin(user_id_map) & edges["post_id"].isin(post_id_map)].copy()
edges["user_idx"] = edges["user_id"].map(user_id_map)
edges["post_idx"] = edges["post_id"].map(post_id_map)

# --- Export for PyTorch Geometric ---
user_x = user_features.values.astype(np.float32)
post_x = post_features.values.astype(np.float32)
edge_index = np.vstack([edges["user_idx"].values, edges["post_idx"].values])
edge_attr = edges[["strength"]].values.astype(np.float32)

user_x_tensor = torch.tensor(user_x)
post_x_tensor = torch.tensor(post_x)
edge_index_tensor = torch.tensor(edge_index, dtype=torch.long)
edge_attr_tensor = torch.tensor(edge_attr)

# --- Time-Based Train/Validation/Test Split ---
edges_sorted = edges.sort_values("timestamp")
train_frac, val_frac = 0.7, 0.2
n = len(edges_sorted)
train_cutoff_time = edges_sorted.iloc[int(n*train_frac)]["timestamp"]
val_cutoff_time = edges_sorted.iloc[int(n*(train_frac+val_frac))]["timestamp"]

train_edges = edges_sorted[edges_sorted["timestamp"] <= train_cutoff_time]
val_edges = edges_sorted[(edges_sorted["timestamp"] > train_cutoff_time) &
                         (edges_sorted["timestamp"] <= val_cutoff_time)]
test_edges = edges_sorted[edges_sorted["timestamp"] > val_cutoff_time]

# --- Create HeteroData Object ---
print("Creating HeteroData object...")
data = HeteroData()
data["user"].x = user_x_tensor
data["post"].x = post_x_tensor
data["user"].node_id = torch.tensor(list(user_id_map.keys()), dtype=torch.long)
data["post"].node_id = torch.tensor(list(post_id_map.keys()), dtype=torch.long)

# Add edge splits
for split_name, split_edges in [("train", train_edges), ("val", val_edges), ("test", test_edges)]:
    user_idx_array = split_edges["user_idx"].values
    post_idx_array = split_edges["post_idx"].values
    split_edge_index = torch.tensor(np.vstack([user_idx_array, post_idx_array]), dtype=torch.long)
    split_edge_attr = torch.tensor(split_edges[["strength"]].values, dtype=torch.float)

    if split_name == "train":
        data["user", "interacts", "post"].edge_index = split_edge_index
        data["user", "interacts", "post"].edge_attr = split_edge_attr
    else:
        data["user", f"{split_name}_interacts", "post"].edge_index = split_edge_index
        data["user", f"{split_name}_interacts", "post"].edge_attr = split_edge_attr

# --- Add Reverse Edges ---
def add_reverse_edges(source_edge_type, target_edge_type):
    data[target_edge_type].edge_index = torch.stack([
        data[source_edge_type].edge_index[1],
        data[source_edge_type].edge_index[0]
    ], dim=0)
    data[target_edge_type].edge_attr = data[source_edge_type].edge_attr.clone()

add_reverse_edges(("user", "interacts", "post"), ("post", "rev_interacts", "user"))
add_reverse_edges(("user", "val_interacts", "post"), ("post", "val_rev_interacts", "user"))
add_reverse_edges(("user", "test_interacts", "post"), ("post", "test_rev_interacts", "user"))

# --- Add Similarity Edges ---
def add_similarity_edges(node_type, features, threshold=0.5):
    sim = cosine_similarity(features)
    sim_edges = np.array(np.where(sim > threshold))
    mask = sim_edges[0] != sim_edges[1]
    sim_edges = sim_edges[:, mask]

    edge_index = torch.tensor(sim_edges, dtype=torch.long)
    edge_attr = torch.tensor(sim[sim_edges[0], sim_edges[1]], dtype=torch.float).unsqueeze(1)

    data[node_type, "similar_to", node_type].edge_index = edge_index
    data[node_type, "similar_to", node_type].edge_attr = edge_attr

print("Calculating similarities...")
add_similarity_edges("user", user_x)
add_similarity_edges("post", post_x)

# --- Feature Normalization ---
def normalize_features(data):
    # Normalize user features (z-score)
    user_x = data['user'].x
    user_mean = user_x.mean(dim=0, keepdim=True)
    user_std = user_x.std(dim=0, keepdim=True) + 1e-8
    data['user'].x = (user_x - user_mean) / user_std

    # Normalize post features
    post_x = data['post'].x
    category_ids = post_x[:, 0].clone()
    numeric_features = post_x[:, 1:]

    # Log transform count features
    numeric_features[:, 0] = torch.log1p(numeric_features[:, 0])  # view_count
    numeric_features[:, 1] = torch.log1p(numeric_features[:, 1])  # upvote_count

    # Min-max normalize ratings if present
    if numeric_features.shape[1] > 2:
        rating_col = 2
        max_rating = numeric_features[:, rating_col].max()
        if max_rating > 0:
            numeric_features[:, rating_col] = numeric_features[:, rating_col] / max_rating

    # Combine features
    data['post'].x = torch.cat([category_ids.unsqueeze(1), numeric_features], dim=1)

    # Normalize edge attributes
    for edge_type in data.edge_types:
        if hasattr(data[edge_type], 'edge_attr') and data[edge_type].edge_attr is not None:
            edge_attr = data[edge_type].edge_attr
            max_val = edge_attr.max()
            if max_val > 0:
                data[edge_type].edge_attr = edge_attr / max_val

    return data

print("Normalizing features...")
data = normalize_features(data)

# --- Move to Device ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
data = data.to(device)

# --- Final Verification ---
print("\nFinal Data Structure:")
print("User features shape:", data["user"].x.shape)
print("Post features shape:", data["post"].x.shape)
print("Edge types:", data.edge_types)
print("Edge attribute ranges:")
for edge_type in data.edge_types:
    if hasattr(data[edge_type], 'edge_attr'):
        print(f"{edge_type}: {data[edge_type].edge_attr.min().item():.2f} - {data[edge_type].edge_attr.max().item():.2f}")

# --- Save Data to Disk ---
def save_torch_data(data, directory="data", filename=None):
    """Save PyTorch HeteroData object to disk"""
    # Create directory if it doesn't exist
    os.makedirs(directory, exist_ok=True)

    # Generate filename with timestamp if not provided
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"social_network_data_{timestamp}.pt"

    # Ensure filename has .pt extension
    if not filename.endswith('.pt'):
        filename += '.pt'

    # Full path
    filepath = os.path.join(directory, filename)

    # Save to CPU first (makes it more portable)
    cpu_data = data.cpu()

    # Save data
    torch.save(cpu_data, filepath)
    print(f"\nData successfully saved to: {filepath}")

    # Save metadata (optional)
    meta_info = {
        'num_users': data["user"].x.shape[0],
        'num_posts': data["post"].x.shape[0],
        'edge_types': data.edge_types,
        'timestamp': datetime.now().isoformat()
    }

    meta_filepath = filepath.replace('.pt', '_meta.pt')
    torch.save(meta_info, meta_filepath)
    print(f"Metadata saved to: {meta_filepath}")

    return filepath

# Save the graph data
save_path = save_torch_data(data)

print("\nHeteroData object successfully created, normalized, and saved!")

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import HeteroConv, GCNConv, SAGEConv, GATConv, Linear
from torch_geometric.data import HeteroData
from torch_geometric.loader import LinkNeighborLoader
import numpy as np
import logging
import json
import os
import time
from typing import Dict, List, Optional, Tuple, Union
from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve
from tqdm import tqdm

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Cache directory for storing recommendations
CACHE_DIR = "./cache"
os.makedirs(CACHE_DIR, exist_ok=True)


class MoodEmbedding(nn.Module):
    """
    Embedding layer for user moods to handle cold start problem
    """
    def __init__(self, num_moods: int, embed_dim: int):
        super().__init__()
        self.embedding = nn.Embedding(num_moods + 1, embed_dim) # +1 for padding/unknown
        # Initialize weights with Xavier initialization
        nn.init.xavier_uniform_(self.embedding.weight)

    def forward(self, mood_ids):
        # Ensure ids are within bounds
        mood_ids = torch.clamp(mood_ids, 0, self.embedding.num_embeddings - 1)
        return self.embedding(mood_ids)

class ContentEmbedding(nn.Module):
    """
    Embedding layer for content features with proper normalization
    """
    def __init__(self, num_categories: int, embed_dim: int):
        super().__init__()
        self.num_categories = num_categories
        self.category_embedding = nn.Embedding(num_categories + 1, embed_dim, padding_idx=0)
        nn.init.xavier_uniform_(self.category_embedding.weight)

        # Add batch normalization for numerical features
        self.batch_norm = nn.BatchNorm1d(3)  # For the 3 numerical features

    def forward(self, x):
        # Assuming first feature is category_id
        category_ids = x[:, 0].long()

        # Ensure ids are within bounds
        category_ids = torch.clamp(category_ids, 0, self.num_categories)
        category_embed = self.category_embedding(category_ids)

        # Normalize other features
        other_features = x[:, 1:]
        if other_features.shape[0] > 1:  # Only apply batch norm if we have more than 1 sample
            other_features = self.batch_norm(other_features)

        return torch.cat([category_embed, other_features], dim=1)

class HeteroGNNRecommender(nn.Module):
    """
    Heterogeneous GNN for video recommendations with improved stability
    """
    def __init__(
        self,
        user_feature_dim: int,
        post_feature_dim: int,
        hidden_dim: int = 64,
        num_layers: int = 2,
        dropout: float = 0.2,
        num_moods: int = 10,
        num_categories: int = 50
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.dropout = dropout
        self.num_categories = num_categories

        # Feature embeddings with batch normalization
        self.user_embedding = nn.Sequential(
            nn.Linear(user_feature_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        self.post_embedding = nn.Sequential(
            ContentEmbedding(self.num_categories, hidden_dim // 2),
            nn.Linear(post_feature_dim + (hidden_dim // 2) - 1, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # Mood embedding for cold start
        self.mood_embedding = MoodEmbedding(num_moods, hidden_dim)

        # Graph Convolution Layers with proper initialization
        self.convs = nn.ModuleList()
        for _ in range(num_layers):
            conv = HeteroConv({
                ('user', 'interacts', 'post'): GATConv((-1, -1), hidden_dim // 2, heads=2, dropout=dropout, add_self_loops=False, concat=True),
                ('post', 'rev_interacts', 'user'): GATConv((-1, -1), hidden_dim // 2, heads=2, dropout=dropout, add_self_loops=False, concat=True),
                ('user', 'similar_to', 'user'): SAGEConv((-1, -1), hidden_dim),
                ('post', 'similar_to', 'post'): SAGEConv((-1, -1), hidden_dim),
            }, aggr='mean')
            self.convs.append(conv)

        # Projection layers with proper initialization
        self.user_proj = nn.Linear(hidden_dim, hidden_dim)
        self.post_proj = nn.Linear(hidden_dim, hidden_dim)

        # Output prediction layers
        self.post_predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 1)
        )

        # Cold start prediction layers
        self.cold_start_predictor = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 1)
        )

        # Initialize weights properly
        self._init_weights()

    def _init_weights(self):
        """Initialize weights using Kaiming initialization"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)

    def forward(self, data: HeteroData):
        x_dict = {
            'user': self.user_embedding(data['user'].x),
            'post': self.post_embedding(data['post'].x)
        }

        for conv in self.convs:
            edge_types = [
                ('user', 'interacts', 'post'),
                ('post', 'rev_interacts', 'user'),
                ('user', 'similar_to', 'user'),
                ('post', 'similar_to', 'post')
            ]

            edge_index_dict = data.edge_index_dict.copy()
            # Remove invalid edge types that do not exist in data
            edge_index_dict = {
                edge_type: edge_index_dict[edge_type] for edge_type in edge_types if edge_type in edge_index_dict
            }

            x_dict = conv(x_dict, edge_index_dict)
            x_dict = {key: F.relu(x) for key, x in x_dict.items()}
            x_dict = {key: F.dropout(x, p=self.dropout, training=self.training) for key, x in x_dict.items()}

        return x_dict

    def recommend(self, data: HeteroData, user_idx: int, top_k: int = 10, exclude_seen: bool = True) -> List[Tuple[int, float]]:
        self.eval()
        with torch.no_grad():
            x_dict = self(data)
            user_emb = x_dict['user'][user_idx].unsqueeze(0)
            post_embs = x_dict['post']

            scores = []
            for i, post_emb in enumerate(post_embs):
                score = torch.sigmoid(self.predict_interaction(user_emb, post_emb.unsqueeze(0)))
                scores.append((i, score.item()))

            if exclude_seen and ('user', 'interacts', 'post') in data.edge_index_dict:
                seen_mask = (data.edge_index_dict[('user', 'interacts', 'post')][0] == user_idx)
                seen_posts = data.edge_index_dict[('user', 'interacts', 'post')][1][seen_mask].tolist()
                scores = [(i, s) for i, s in scores if i not in seen_posts]

            scores.sort(key=lambda x: x[1], reverse=True)
            return scores[:top_k]

    def cold_start_recommend(self, data: HeteroData, mood_id: int, top_k: int = 10) -> List[Tuple[int, float]]:
        self.eval()
        with torch.no_grad():
            x_dict = self(data)
            mood_id_tensor = torch.tensor([mood_id], device=data['post'].x.device)
            mood_emb = self.mood_embedding(mood_id_tensor)
            post_embs = x_dict['post']

            # Normalize both mood and post embeddings before concatenation
            mood_emb = F.normalize(mood_emb, p=2, dim=-1)
            post_embs = F.normalize(post_embs, p=2, dim=-1)

            scores = []
            for i, post_emb in enumerate(post_embs):
                combined = torch.cat([mood_emb, post_emb.unsqueeze(0)], dim=1)
                score = torch.sigmoid(self.cold_start_predictor(combined))
                scores.append((i, score.item()))

            scores.sort(key=lambda x: x[1], reverse=True)
            return scores[:top_k]

    def predict_interaction(self, user_emb: torch.Tensor, post_emb: torch.Tensor) -> torch.Tensor:
        """Project user and post embeddings to the same dimension, multiply, and predict interaction"""
        user_proj = self.user_proj(user_emb)
        post_proj = self.post_proj(post_emb)

        # Element-wise multiplication (Hadamard product)
        interaction = user_proj * post_proj

        return self.post_predictor(interaction)

class VideoRecommendationSystem:
    """
    Complete video recommendation system integrating the GNN model
    with caching and API integration
    """
    def __init__(
        self,
        model: HeteroGNNRecommender,
        data: HeteroData,
        user_id_map: Dict,
        post_id_map: Dict,
        cache_ttl: int = 3600,  # Cache time to live (seconds)
        page_size: int = 20
    ):
        self.model = model
        self.data = data
        self.user_id_map = user_id_map  # Maps external user IDs to internal indices
        self.post_id_map = post_id_map  # Maps external post IDs to internal indices
        self.reverse_user_map = {v: k for k, v in user_id_map.items()}
        self.reverse_post_map = {v: k for k, v in post_id_map.items()}
        self.cache_ttl = cache_ttl
        self.page_size = page_size

        # Initialize cache
        self.recommendation_cache = {}

    def get_recommendations(
        self,
        user_id: str,
        page: int = 1,
        refresh_cache: bool = False,
        mood_id: Optional[int] = None
    ) -> Dict:
        """
        Get paginated recommendations for a user
        """
        # Check if user exists
        is_cold_start = user_id not in self.user_id_map

        # Handle cold start with mood-based recommendations
        if is_cold_start:
            if mood_id is None:
                raise ValueError("Mood ID required for new users")
            return self._get_cold_start_recommendations(mood_id, page)

        # Convert external user ID to internal index
        user_idx = self.user_id_map.get(user_id)

        # Create cache key
        cache_key = f"{user_id}_{page}"

        # Check cache
        if not refresh_cache and cache_key in self.recommendation_cache:
            cached_data = self.recommendation_cache[cache_key]
            if time.time() - cached_data["timestamp"] < self.cache_ttl:
                logger.info(f"Using cached recommendations for user {user_id}, page {page}")
                return cached_data["recommendations"]

        # Generate recommendations
        try:
            with torch.no_grad():
                # Get top recommendations
                all_recs = self.model.recommend(
                    self.data,
                    user_idx,
                    top_k=self.page_size * page * 2,  # Get more than needed for pagination
                    exclude_seen=True
                )

                # Apply pagination
                start_idx = (page - 1) * self.page_size
                end_idx = min(start_idx + self.page_size, len(all_recs))
                paged_recs = all_recs[start_idx:end_idx]

                # Convert internal indices to external IDs
                recommendations = []
                for post_idx, score in paged_recs:
                    post_id = self.reverse_post_map[post_idx]
                    recommendations.append({
                        "post_id": post_id,
                        "score": score,
                        "rank": len(recommendations) + 1 + start_idx
                    })

                # Create response
                response = {
                    "user_id": user_id,
                    "page": page,
                    "page_size": self.page_size,
                    "total_items": len(all_recs),
                    "recommendations": recommendations
                }

                # Cache the results
                self.recommendation_cache[cache_key] = {
                    "timestamp": time.time(),
                    "recommendations": response
                }

                return response

        except Exception as e:
            logger.error(f"Error generating recommendations: {str(e)}")
            raise

    def _get_cold_start_recommendations(self, mood_id: int, page: int = 1) -> Dict:
        """
        Get recommendations for a new user based on mood
        """
        # Create cache key
        cache_key = f"mood_{mood_id}_{page}"

        # Check cache
        if cache_key in self.recommendation_cache:
            cached_data = self.recommendation_cache[cache_key]
            if time.time() - cached_data["timestamp"] < self.cache_ttl:
                logger.info(f"Using cached mood recommendations for mood {mood_id}, page {page}")
                return cached_data["recommendations"]

        # Generate recommendations
        try:
            with torch.no_grad():
                # Get top recommendations based on mood
                all_recs = self.model.cold_start_recommend(
                    self.data,
                    mood_id,
                    top_k=self.page_size * page * 2  # Get more than needed for pagination
                )

                # Apply pagination
                start_idx = (page - 1) * self.page_size
                end_idx = min(start_idx + self.page_size, len(all_recs))
                paged_recs = all_recs[start_idx:end_idx]

                # Convert internal indices to external IDs
                recommendations = []
                for post_idx, score in paged_recs:
                    post_id = self.reverse_post_map[post_idx]
                    recommendations.append({
                        "post_id": post_id,
                        "score": score,
                        "rank": len(recommendations) + 1 + start_idx
                    })

                # Create response
                response = {
                    "mood_id": mood_id,
                    "page": page,
                    "page_size": self.page_size,
                    "total_items": len(all_recs),
                    "recommendations": recommendations
                }

                # Cache the results
                self.recommendation_cache[cache_key] = {
                    "timestamp": time.time(),
                    "recommendations": response
                }

                return response

        except Exception as e:
            logger.error(f"Error generating mood-based recommendations: {str(e)}")
            raise

    def update_user_interaction(self, user_id: str, post_id: str, interaction_type: str, strength: float):
          """
          Update the graph with a new user interaction and invalidate cache
          """
          # Convert external IDs to internal indices
          user_idx = self.user_id_map.get(user_id)
          post_idx = self.post_id_map.get(post_id)

          if user_idx is None or post_idx is None:
              logger.warning(f"User {user_id} or post {post_id} not found in maps")
              return False

          # Update the data (this part should be implemented based on specific heterogeneous graph structure)
          # Placeholder for actual implementation
          logger.info(f"Updating interaction: user {user_id} -> {interaction_type} -> post {post_id}")

          # Invalidate all relevant caches
          cache_keys_to_remove = [key for key in self.recommendation_cache if key.startswith(f"{user_id}_")]

          # Additionally, remove mood-based caches that could include content for this user
          for mood_id in self.user_id_map.values():
              mood_cache_key = f"mood_{mood_id}_1"  # Example: checking cache for mood 1, adjust as needed
              cache_keys_to_remove.append(mood_cache_key)

          # Remove the identified cache keys
          for key in cache_keys_to_remove:
              if key in self.recommendation_cache:
                  del self.recommendation_cache[key]

          logger.info(f"Cache invalidated for user {user_id} and related mood recommendations")

          return True

import torch
import torch.nn.functional as F
import numpy as np
import logging
from sklearn.metrics import roc_auc_score, average_precision_score, precision_score, recall_score, f1_score

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
from sklearn.metrics import roc_auc_score, average_precision_score
import torch.optim.lr_scheduler as lr_scheduler


def train_model(
    model,
    data,
    batch_size=64,
    learning_rate=0.0001,  # Reduced learning rate for stability
    weight_decay=1e-5,
    num_epochs=30,
    patience=5,
    device='cpu'
):
    print(f"Training model on {device}")
    model = model.to(device)
    data = data.to(device)

    # Create data loaders with properly normalized features
    train_loader, val_loader, test_loader = create_data_loaders(data, batch_size=batch_size)

    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)

    # Use BCEWithLogitsLoss for numerical stability
    criterion = nn.BCEWithLogitsLoss()

    best_val_loss = float('inf')
    best_model_state = model.state_dict().copy()
    patience_counter = 0
    train_losses, val_losses = [], []

    for epoch in range(num_epochs):
        model.train()
        epoch_train_loss = 0.0
        num_train_batches = 0

        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} (Train)")
        for batch_idx, (user_ids, post_ids, strengths) in enumerate(progress_bar):
            # Ensure strengths are properly scaled to [0,1]
            strengths = torch.clamp(strengths, 0, 1)

            optimizer.zero_grad()

            # Forward pass through GNN
            x_dict = model(data)

            # Get embeddings for specific users and posts in this batch
            user_emb = x_dict['user'][user_ids.long()]
            post_emb = x_dict['post'][post_ids.long()]

            # Predict interaction scores (logits)
            predictions = model.predict_interaction(user_emb, post_emb)

            # Calculate loss with BCEWithLogitsLoss
            loss = criterion(predictions.squeeze(), strengths)

            # Backward pass and optimization
            loss.backward()

            # Add gradient clipping for stability
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            epoch_train_loss += loss.item()
            num_train_batches += 1

            # Update progress bar
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

        avg_train_loss = epoch_train_loss / num_train_batches
        train_losses.append(avg_train_loss)

        # Validation phase
        model.eval()
        epoch_val_loss = 0.0
        num_val_batches = 0

        with torch.no_grad():
            val_progress = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} (Val)")
            for user_ids, post_ids, strengths in val_progress:
                # Ensure strengths are properly scaled to [0,1]
                strengths = torch.clamp(strengths, 0, 1)

                x_dict = model(data)
                user_emb = x_dict['user'][user_ids.long()]
                post_emb = x_dict['post'][post_ids.long()]

                predictions = model.predict_interaction(user_emb, post_emb)
                val_loss = criterion(predictions.squeeze(), strengths)

                epoch_val_loss += val_loss.item()
                num_val_batches += 1

                val_progress.set_postfix({'val_loss': f'{val_loss.item():.4f}'})

        avg_val_loss = epoch_val_loss / num_val_batches
        val_losses.append(avg_val_loss)

        scheduler.step(avg_val_loss)  # Update learning rate

        print(f"[Epoch {epoch+1}/{num_epochs}] Train Loss: {avg_train_loss:.4f}")
        print(f"[Epoch {epoch+1}/{num_epochs}] Validation Loss: {avg_val_loss:.4f}")
        print(f"Current LR: {optimizer.param_groups[0]['lr']:.2e}")

        # Early stopping and model checkpointing
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model_state = model.state_dict().copy()
            patience_counter = 0
            print(f"New best model (val loss: {best_val_loss:.4f})")
        else:
            patience_counter += 1
            print(f"No improvement for {patience_counter} epochs")

            if patience_counter >= patience:
                print(f"Early stopping triggered after {epoch+1} epochs")
                break

    # Load best model
    model.load_state_dict(best_model_state)

    # Final Evaluation with proper metrics
    model.eval()
    test_loss = 0.0
    num_test_batches = 0
    predictions_list, targets_list = [], []

    with torch.no_grad():
        test_progress = tqdm(test_loader, desc="Final Evaluation")
        for user_ids, post_ids, strengths in test_progress:
            # Ensure strengths are properly scaled to [0,1]
            strengths = torch.clamp(strengths, 0, 1)

            x_dict = model(data)
            user_emb = x_dict['user'][user_ids.long()]
            post_emb = x_dict['post'][post_ids.long()]

            predictions = model.predict_interaction(user_emb, post_emb)
            test_loss += criterion(predictions.squeeze(), strengths).item()
            num_test_batches += 1

            # Store predictions and targets for metrics
            predictions_list.append(torch.sigmoid(predictions).cpu().numpy())
            targets_list.append(strengths.cpu().numpy())

    # Calculate metrics
    all_predictions = np.concatenate([p.flatten() for p in predictions_list])
    all_targets = np.concatenate([t.flatten() for t in targets_list])
    all_targets = (all_targets > 0.5).astype(int)
    # Convert to binary predictions using optimal threshold
    precision, recall, thresholds = precision_recall_curve(all_targets, all_predictions)
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)
    optimal_threshold = thresholds[np.argmax(f1_scores)]
    binary_predictions = (all_predictions > optimal_threshold).astype(int)

    metrics = {
        "test_loss": test_loss / num_test_batches,
        "mae": np.mean(np.abs(all_predictions - all_targets)),
        "rmse": np.sqrt(np.mean((all_predictions - all_targets) ** 2)),
        "auc": roc_auc_score(all_targets, all_predictions)
    }

    return model, metrics

def save_model_to_h(model, filename="model_weights.txt"):
    """
    Save all model weights as C arrays in a .h file.

    Args:
        model (torch.nn.Module): Trained PyTorch model.
        filename (str): Output .h file name.
    """
    import numpy as np

    with open(filename, "w") as f:
        f.write("// Auto-generated model weights\n\n")
        for name, param in model.state_dict().items():
            arr = param.cpu().numpy().flatten()
            arr_str = ', '.join([f"{x:.8f}f" for x in arr])
            ctype = "float"
            var_name = name.replace('.', '_')
            f.write(f"const {ctype} {var_name}[{len(arr)}] = {{ {arr_str} }};\n\n")

    print(f"Model weights saved to {filename}")

# !pip install pyg-lib -f https://data.pyg.org/whl/torch-2.6.0+cu124.html
# !pip install torch-sparse -f https://data.pyg.org/whl/torch-2.6.0+cu124.html

def prepare_data_splits(data, train_ratio=0.7, val_ratio=0.2):
    """
    Create train/val/test edge types for the heterogeneous graph.
    Consistently uses new edge types for splitting.
    """
    if ("user", "val_interacts", "post") not in data.edge_types:
        print("Creating train/val/test splits...")

        # Original edge
        edge_index = data["user", "interacts", "post"].edge_index
        edge_attr = data["user", "interacts", "post"].edge_attr

        num_edges = edge_index.size(1)

        # Shuffle indices
        perm = torch.randperm(num_edges)
        train_end = int(train_ratio * num_edges)
        val_end = int((train_ratio + val_ratio) * num_edges)

        train_idx = perm[:train_end]
        val_idx = perm[train_end:val_end]
        test_idx = perm[val_end:]

        # Assign to new edge types
        data["user", "val_interacts", "post"].edge_index = edge_index[:, val_idx]
        data["user", "val_interacts", "post"].edge_attr = edge_attr[val_idx]

        data["user", "test_interacts", "post"].edge_index = edge_index[:, test_idx]
        data["user", "test_interacts", "post"].edge_attr = edge_attr[test_idx]

        # Overwrite training edges in-place
        data["user", "interacts", "post"].edge_index = edge_index[:, train_idx]
        data["user", "interacts", "post"].edge_attr = edge_attr[train_idx]

        print(f"Split sizes - Train: {train_idx.size(0)}, Val: {val_idx.size(0)}, Test: {test_idx.size(0)}")

    return data

import torch
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F
import logging
from torch_geometric.data import HeteroData


def create_data_loaders(data, batch_size=32, num_workers=0):
    """
    Create PyTorch DataLoaders for training the GNN.
    Properly extracts edge indices to build datasets for batch training.
    """
    # Extract edge indices and attributes for each split
    train_edge_index = data["user", "interacts", "post"].edge_index
    train_edge_attr = data["user", "interacts", "post"].edge_attr.squeeze()

    val_edge_index = data["user", "val_interacts", "post"].edge_index
    val_edge_attr = data["user", "val_interacts", "post"].edge_attr.squeeze()

    test_edge_index = data["user", "test_interacts", "post"].edge_index
    test_edge_attr = data["user", "test_interacts", "post"].edge_attr.squeeze()

    # Normalize edge attributes to [0,1] range for stability
    train_edge_attr = torch.clamp(train_edge_attr / train_edge_attr.max(), 0, 1)
    val_edge_attr = torch.clamp(val_edge_attr / val_edge_attr.max(), 0, 1)
    test_edge_attr = torch.clamp(test_edge_attr / test_edge_attr.max(), 0, 1)

    # Create tensors for source (user) and destination (post) indices
    train_src_indices = train_edge_index[0]
    train_dst_indices = train_edge_index[1]

    val_src_indices = val_edge_index[0]
    val_dst_indices = val_edge_index[1]

    test_src_indices = test_edge_index[0]
    test_dst_indices = test_edge_index[1]

    # Create TensorDatasets
    train_dataset = torch.utils.data.TensorDataset(train_src_indices, train_dst_indices, train_edge_attr)
    val_dataset = torch.utils.data.TensorDataset(val_src_indices, val_dst_indices, val_edge_attr)
    test_dataset = torch.utils.data.TensorDataset(test_src_indices, test_dst_indices, test_edge_attr)

    # Create DataLoaders
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)

    return train_loader, val_loader, test_loader


# Example usage
if __name__ == "__main__":
    # Assume data is already loaded
    print(f"User features shape: {data['user'].x.shape}")
    print(f"Post features shape: {data['post'].x.shape}")
    print(f"User-post interactions: {data['user', 'interacts', 'post'].edge_index.shape[1]}")

    # Check the actual max category ID in the data
    max_category_id = int(data['post'].x[:, 0].max().item())
    print(f"Maximum category ID in data: {max_category_id}")

    # Create the model with appropriate parameters
    model = HeteroGNNRecommender(
        user_feature_dim=data['user'].x.shape[1],
        post_feature_dim=data['post'].x.shape[1],
        hidden_dim=256,
        num_layers=4,
        dropout=0.2,
        num_moods=4,
        num_categories=13
    )

    # Split data into train/val/test sets if not already done
    if not hasattr(data['user', 'interacts', 'post'], 'train_edge_index'):
        # If data doesn't have train/val/test splits, we need to create them
        edge_index = data['user', 'interacts', 'post'].edge_index
        num_edges = edge_index.size(1)

        # Create random permutation
        perm = torch.randperm(num_edges)
        train_idx = perm[:int(0.8 * num_edges)]
        val_idx = perm[int(0.8 * num_edges):int(0.9 * num_edges)]
        test_idx = perm[int(0.9 * num_edges):]

        # Assign edges to train/val/test
        data['user', 'interacts', 'post'].train_edge_index = edge_index[:, train_idx]
        data['user', 'interacts', 'post'].val_edge_index = edge_index[:, val_idx]
        data['user', 'interacts', 'post'].test_edge_index = edge_index[:, test_idx]

        logger.info(f"Created data splits: {train_idx.size(0)} train, {val_idx.size(0)} val, {test_idx.size(0)} test edges")

    # Train the model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    trained_model, metrics = train_model(
        model=model,
        data=data,
        batch_size=64,
        learning_rate=0.0001,
        num_epochs=10,
        patience=3,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    torch.save(trained_model.state_dict(), "model.pth")  #
    save_model_to_h(trained_model, "model_weights.txt")

    # Print final metrics
    print("\nFinal Evaluation Metrics:")
    for metric_name, value in metrics.items():
        print(f"{metric_name}: {value:.4f}")

    # Create recommendation system
    num_users = data['user'].x.shape[0]
    num_posts = data['post'].x.shape[0]

    user_id_map = {f"user_{i}": i for i in range(num_users)}
    post_id_map = {f"post_{i}": i for i in range(num_posts)}

    rec_system = VideoRecommendationSystem(
        model=trained_model,
        data=data,
        user_id_map=user_id_map,
        post_id_map=post_id_map,
        cache_ttl=3600,
        page_size=10
    )


    print("\nTraining and evaluation complete!")

