{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T22:23:53.677833Z",
     "start_time": "2024-12-05T22:23:53.675702Z"
    }
   },
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Flask app initialization\n",
    "app = Flask(__name__)\n",
    "\n",
    "BASE_URL = \"https://api.socialverseapp.com\"\n",
    "HEADERS = {\n",
    "    \"Flic-Token\": \"flic_6e2d8d25dc29a4ddd382c2383a903cf4a688d1a117f6eb43b35a1e7fadbb84b8\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T22:23:53.682599Z",
     "start_time": "2024-12-05T22:23:53.679545Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Data Fetching and Preprocessing ---\n",
    "def fetch_data(endpoint, max_pages=10, extra_params=None):\n",
    "    \"\"\"\n",
    "    Fetch paginated data from a given API endpoint.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    page = 1\n",
    "\n",
    "    if extra_params is None:\n",
    "        extra_params = {}\n",
    "\n",
    "    while page <= max_pages:\n",
    "        # Add pagination parameters\n",
    "        params = {\"page\": page, \"page_size\": 1000}\n",
    "        params.update(extra_params)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(f\"{BASE_URL}/{endpoint}\", headers=HEADERS, params=params)\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "            # Extract the 'posts' key from the response JSON\n",
    "            page_data = response.json().get('posts', [])\n",
    "            if not page_data:  # Stop if no 'posts' data is returned\n",
    "                page_data = response.json().get('users', [])\n",
    "            if not page_data:   \n",
    "                break\n",
    "            \n",
    "            data.extend(page_data)\n",
    "            page += 1\n",
    "        except requests.RequestException as e:\n",
    "            logger.error(f\"Error fetching data from {endpoint}: {e}\")\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T22:24:13.865607Z",
     "start_time": "2024-12-05T22:23:53.689770Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load and preprocess data from various endpoints\n",
    "    \"\"\"\n",
    "    global all_users, all_posts, rated_posts\n",
    "    extra_params = {\n",
    "        \"resonance_algorithm\": \"resonance_algorithm_cjsvervb7dbhss8bdrj89s44jfjdbsjd0xnjkbvuire8zcjwerui3njfbvsujc5if\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        viewed_posts = fetch_data(endpoint=\"posts/view\", extra_params=extra_params)\n",
    "        liked_posts = fetch_data(endpoint=\"posts/like\", extra_params=extra_params)\n",
    "        inspired_posts = fetch_data(endpoint=\"posts/inspire\", extra_params=extra_params)\n",
    "        rated_posts = fetch_data(endpoint=\"posts/rating\", extra_params=extra_params)\n",
    "        all_posts = fetch_data(endpoint=\"posts/summary/get\")\n",
    "        all_users = fetch_data(endpoint=\"users/get_all\")\n",
    "\n",
    "        # Rename id columns\n",
    "        all_posts.rename(columns={'id': 'post_id'}, inplace=True)\n",
    "        all_users.rename(columns={'id': 'user_id'}, inplace=True)\n",
    "\n",
    "        # Merge interactions\n",
    "        interactions = pd.concat([viewed_posts, liked_posts, inspired_posts, rated_posts])\n",
    "        \n",
    "        # Merge with metadata\n",
    "        posts_with_metadata = pd.merge(interactions, all_posts, on='post_id', how='inner')\n",
    "        user_data = pd.merge(posts_with_metadata, all_users, on='user_id', how='inner')\n",
    "\n",
    "        logger.info(f\"Total users: {len(all_users)}\")\n",
    "        logger.info(f\"Total posts: {len(all_posts)}\")\n",
    "        logger.info(f\"Total interactions: {len(interactions)}\")\n",
    "        logger.info(f\"All posts columns: {all_posts.columns.tolist()}\")\n",
    "\n",
    "        return user_data, all_posts, rated_posts\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in data loading: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and Model Training\n",
    "def preprocess_data(user_data, all_posts):\n",
    "    \"\"\"\n",
    "    Preprocess data for recommendation system\n",
    "    \"\"\"\n",
    "    # Numeric feature scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    numeric_features = ['upvote_count', 'view_count', 'rating_percent', 'average_rating']\n",
    "    user_data[numeric_features] = scaler.fit_transform(user_data[numeric_features])\n",
    "\n",
    "    # Text preprocessing\n",
    "    all_posts['title'] = all_posts['title'].fillna('').astype(str)\n",
    "    all_posts['post_summary'] = all_posts['post_summary'].fillna('').astype(str)\n",
    "\n",
    "    # Vectorization\n",
    "    tfidf = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "    content_combined = all_posts['title'] + ' ' + all_posts['post_summary']\n",
    "    all_posts['content_vector'] = list(tfidf.fit_transform(content_combined).toarray())\n",
    "    \n",
    "    # Similarity matrix\n",
    "    similarity_matrix = cosine_similarity(np.array(all_posts['content_vector'].tolist()))\n",
    "\n",
    "    return user_data, all_posts, similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_recommendation_model(rated_posts):\n",
    "    \"\"\"\n",
    "    Train collaborative filtering model\n",
    "    \"\"\"\n",
    "    reader = Reader(rating_scale=(0, 100))\n",
    "    data = Dataset.load_from_df(rated_posts[['user_id', 'post_id', 'rating_percent']], reader)\n",
    "    \n",
    "    # Split data for training and testing\n",
    "    trainset, testset = train_test_split(data, test_size=0.2)\n",
    "    \n",
    "    # Train SVD model\n",
    "    model = SVD()\n",
    "    model.fit(trainset)\n",
    "    \n",
    "    return model, trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 15:50:47,073 - ERROR - Error fetching data from posts/view: HTTPSConnectionPool(host='api.socialverseapp.com', port=443): Max retries exceeded with url: /posts/view?page=1&page_size=1000&resonance_algorithm=resonance_algorithm_cjsvervb7dbhss8bdrj89s44jfjdbsjd0xnjkbvuire8zcjwerui3njfbvsujc5if (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x16be39ee0>: Failed to resolve 'api.socialverseapp.com' ([Errno 8] nodename nor servname provided, or not known)\"))\n",
      "2024-12-08 15:50:47,075 - ERROR - Error fetching data from posts/like: HTTPSConnectionPool(host='api.socialverseapp.com', port=443): Max retries exceeded with url: /posts/like?page=1&page_size=1000&resonance_algorithm=resonance_algorithm_cjsvervb7dbhss8bdrj89s44jfjdbsjd0xnjkbvuire8zcjwerui3njfbvsujc5if (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x176237530>: Failed to resolve 'api.socialverseapp.com' ([Errno 8] nodename nor servname provided, or not known)\"))\n",
      "2024-12-08 15:50:47,078 - ERROR - Error fetching data from posts/inspire: HTTPSConnectionPool(host='api.socialverseapp.com', port=443): Max retries exceeded with url: /posts/inspire?page=1&page_size=1000&resonance_algorithm=resonance_algorithm_cjsvervb7dbhss8bdrj89s44jfjdbsjd0xnjkbvuire8zcjwerui3njfbvsujc5if (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x176173200>: Failed to resolve 'api.socialverseapp.com' ([Errno 8] nodename nor servname provided, or not known)\"))\n",
      "2024-12-08 15:50:47,079 - ERROR - Error fetching data from posts/rating: HTTPSConnectionPool(host='api.socialverseapp.com', port=443): Max retries exceeded with url: /posts/rating?page=1&page_size=1000&resonance_algorithm=resonance_algorithm_cjsvervb7dbhss8bdrj89s44jfjdbsjd0xnjkbvuire8zcjwerui3njfbvsujc5if (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x1741f3830>: Failed to resolve 'api.socialverseapp.com' ([Errno 8] nodename nor servname provided, or not known)\"))\n",
      "2024-12-08 15:50:47,081 - ERROR - Error fetching data from posts/summary/get: HTTPSConnectionPool(host='api.socialverseapp.com', port=443): Max retries exceeded with url: /posts/summary/get?page=1&page_size=1000 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x176236ff0>: Failed to resolve 'api.socialverseapp.com' ([Errno 8] nodename nor servname provided, or not known)\"))\n",
      "2024-12-08 15:50:47,084 - ERROR - Error fetching data from users/get_all: HTTPSConnectionPool(host='api.socialverseapp.com', port=443): Max retries exceeded with url: /users/get_all?page=1&page_size=1000 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x1577df860>: Failed to resolve 'api.socialverseapp.com' ([Errno 8] nodename nor servname provided, or not known)\"))\n",
      "2024-12-08 15:50:47,085 - ERROR - Error in data loading: 'post_id'\n",
      "2024-12-08 15:50:47,086 - ERROR - Initialization error: 'post_id'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'post_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/04/3lrlzf7x33xgsx7fr_9yb2k00000gn/T/ipykernel_81557/3161191424.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0muser_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_posts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_posts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_recommendation_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrated_posts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34mInitialization error: \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/04/3lrlzf7x33xgsx7fr_9yb2k00000gn/T/ipykernel_81557/2868027969.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_posts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrated_posts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34mError in data loading: \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_merge_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'post_id'"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "try:\n",
    "    user_data, all_posts, rated_posts = load_data()\n",
    "    user_data, all_posts, similarity_matrix = preprocess_data(user_data, all_posts)\n",
    "    model, trainset, testset = train_recommendation_model(rated_posts)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Initialization error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Recommendation Functions ---\n",
    "def recommend_content_based(user_id, num_recommendations=10):\n",
    "    \"\"\"\n",
    "    Content-based recommendation\n",
    "    \"\"\"\n",
    "    user_interactions = user_data[user_data['user_id'] == user_id]\n",
    "    user_posts = user_interactions['post_id'].unique()\n",
    "\n",
    "    similar_posts = []\n",
    "    for post_id in user_posts:\n",
    "        post_index = all_posts[all_posts['post_id'] == post_id].index[0]\n",
    "        similar_posts += list(enumerate(similarity_matrix[post_index]))\n",
    "\n",
    "    similar_posts = sorted(similar_posts, key=lambda x: x[1], reverse=True)\n",
    "    recommended_post_ids = [\n",
    "        all_posts.iloc[i[0]]['post_id']\n",
    "        for i in similar_posts\n",
    "        if all_posts.iloc[i[0]]['post_id'] not in user_posts\n",
    "    ]\n",
    "    return recommended_post_ids[:num_recommendations]\n",
    "\n",
    "def recommend_collaborative(user_id, num_recommendations=10):\n",
    "    \"\"\"\n",
    "    Collaborative filtering recommendation\n",
    "    \"\"\"\n",
    "    user_rated_posts = rated_posts[rated_posts['user_id'] == user_id]['post_id'].unique()\n",
    "    all_post_ids = all_posts['post_id'].unique()\n",
    "    unrated_posts = [post_id for post_id in all_post_ids if post_id not in user_rated_posts]\n",
    "\n",
    "    predictions = [model.predict(user_id, post_id) for post_id in unrated_posts]\n",
    "    predictions = sorted(predictions, key=lambda x: x.est, reverse=True)\n",
    "\n",
    "    recommended_post_ids = [pred.iid for pred in predictions[:num_recommendations]]\n",
    "    return recommended_post_ids\n",
    "\n",
    "def recommend_hybrid(user_id, category_id=None, mood=None, num_recommendations=10):\n",
    "    \"\"\"\n",
    "    Hybrid recommendation with optional filtering\n",
    "    \"\"\"\n",
    "    filtered_posts = all_posts.copy()\n",
    "    \n",
    "    # Apply optional filters\n",
    "    if category_id:\n",
    "        try:\n",
    "            # Assuming category is a nested dictionary\n",
    "            filtered_posts = filtered_posts[filtered_posts['category'].apply(lambda x: x.get('id') == int(category_id))]\n",
    "        except (KeyError, TypeError) as e:\n",
    "            print(f\"Warning: Could not filter by category. Error: {e}\")\n",
    "    if mood:\n",
    "        if 'mood' in filtered_posts.columns:\n",
    "            filtered_posts = filtered_posts[filtered_posts['mood'] == mood]\n",
    "        else:\n",
    "            logger.warning(\"'mood' column not found in posts data. Skipping mood filter.\")\n",
    "\n",
    "    # Generate recommendations\n",
    "    content_recs = recommend_content_based(user_id, num_recommendations)\n",
    "    collab_recs = recommend_collaborative(user_id, num_recommendations)\n",
    "\n",
    "    # Combine and deduplicate recommendations\n",
    "    recs = list(set(content_recs) | set(collab_recs))\n",
    "    return recs[:num_recommendations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_id(username, all_users):\n",
    "    print(username)\n",
    "    user = all_users[all_users['username'] == username]\n",
    "    if not user.empty:\n",
    "        return user.iloc[0]['user_id']\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1234,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_users['user_id'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 15:34:47,494 - WARNING - 'mood' column not found in posts data. Skipping mood filter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doeyyy\n",
      "[770, 1029, 325, 1288, 1224, 11, 779, 1103, 785, 918]\n"
     ]
    }
   ],
   "source": [
    "user_id = get_user_id(username=\"doeyyy\", all_users=all_users)\n",
    "print(recommend_hybrid(user_id=9, mood=\"happy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'first_name', 'last_name', 'username', 'email', 'role',\n",
       "       'profile_url', 'bio', 'website_url', 'instagram-url', 'youtube_url',\n",
       "       'tictok_url', 'isVerified', 'referral_code', 'has_wallet', 'last_login',\n",
       "       'share_count', 'post_count', 'following_count', 'follower_count',\n",
       "       'is_verified', 'is_online', 'latitude', 'longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_users.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Posts Columns: ['post_id', 'category', 'slug', 'title', 'identifier', 'comment_count', 'upvote_count', 'view_count', 'exit_count', 'rating_count', 'average_rating', 'share_count', 'video_link', 'contract_address', 'chain_id', 'chart_url', 'baseToken', 'is_locked', 'created_at', 'first_name', 'last_name', 'username', 'upvoted', 'bookmarked', 'thumbnail_url', 'gif_thumbnail_url', 'following', 'picture_url', 'post_summary', 'content_vector']\n",
      "User Data Columns: ['id', 'post_id', 'user_id', 'viewed_at', 'liked_at', 'inspired_at', 'rating_percent', 'rated_at', 'category', 'slug', 'title', 'identifier', 'comment_count', 'upvote_count', 'view_count', 'exit_count', 'rating_count', 'average_rating', 'share_count_x', 'video_link', 'contract_address', 'chain_id', 'chart_url', 'baseToken', 'is_locked', 'created_at', 'first_name_x', 'last_name_x', 'username_x', 'upvoted', 'bookmarked', 'thumbnail_url', 'gif_thumbnail_url', 'following', 'picture_url', 'post_summary', 'first_name_y', 'last_name_y', 'username_y', 'email', 'role', 'profile_url', 'bio', 'website_url', 'instagram-url', 'youtube_url', 'tictok_url', 'isVerified', 'referral_code', 'has_wallet', 'last_login', 'share_count_y', 'post_count', 'following_count', 'follower_count', 'is_verified', 'is_online', 'latitude', 'longitude']\n"
     ]
    }
   ],
   "source": [
    "print(\"All Posts Columns:\", all_posts.columns.tolist())\n",
    "print(\"User Data Columns:\", user_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_content_based(user_id, num_recommendations=10):\n",
    "    \"\"\"\n",
    "    Content-based recommendation with similarity scores\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples (post_id, similarity_score)\n",
    "    \"\"\"\n",
    "    user_interactions = user_data[user_data['user_id'] == user_id]\n",
    "    user_posts = user_interactions['post_id'].unique()\n",
    "\n",
    "    similar_posts = []\n",
    "    for post_id in user_posts:\n",
    "        post_index = all_posts[all_posts['post_id'] == post_id].index[0]\n",
    "        # Create list of (post_index, similarity_score) tuples\n",
    "        similar_posts += [(index, score) \n",
    "                          for index, score in enumerate(similarity_matrix[post_index])\n",
    "                          if all_posts.iloc[index]['post_id'] not in user_posts]\n",
    "\n",
    "    # Sort by similarity score in descending order\n",
    "    similar_posts = sorted(similar_posts, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Convert to list of (post_id, similarity_score)\n",
    "    recommended_posts_with_scores = [\n",
    "        (all_posts.iloc[i[0]]['post_id'], i[1])\n",
    "        for i in similar_posts\n",
    "    ]\n",
    "    \n",
    "    return recommended_posts_with_scores[:num_recommendations]\n",
    "\n",
    "def recommend_collaborative(user_id, num_recommendations=10):\n",
    "    \"\"\"\n",
    "    Collaborative filtering recommendation with prediction scores\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples (post_id, prediction_score)\n",
    "    \"\"\"\n",
    "    user_rated_posts = rated_posts[rated_posts['user_id'] == user_id]['post_id'].unique()\n",
    "    all_post_ids = all_posts['post_id'].unique()\n",
    "    unrated_posts = [post_id for post_id in all_post_ids if post_id not in user_rated_posts]\n",
    "\n",
    "    predictions = [\n",
    "        (post_id, model.predict(user_id, post_id).est) \n",
    "        for post_id in unrated_posts\n",
    "    ]\n",
    "    \n",
    "    # Sort predictions by estimated rating in descending order\n",
    "    predictions = sorted(predictions, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return predictions[:num_recommendations]\n",
    "\n",
    "def recommend_hybrid_optimized(user_id, category_id=None, mood=None, num_recommendations=10):\n",
    "    \"\"\"\n",
    "    Enhanced hybrid recommendation with weighted blending of content-based and collaborative filtering.\n",
    "    \n",
    "    Args:\n",
    "        user_id (int): The ID of the user to generate recommendations for\n",
    "        category_id (int, optional): Filter recommendations by specific category\n",
    "        mood (str, optional): Filter recommendations by mood\n",
    "        num_recommendations (int, optional): Number of recommendations to return\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of recommended post IDs\n",
    "    \"\"\"\n",
    "    # Content-based recommendations\n",
    "    content_recs = recommend_content_based(user_id, num_recommendations * 2)\n",
    "    content_scores = dict(content_recs)\n",
    "\n",
    "    # Collaborative filtering recommendations\n",
    "    collab_recs = recommend_collaborative(user_id, num_recommendations * 2)\n",
    "    collab_scores = dict(collab_recs)\n",
    "\n",
    "    # Combine recommendations with weighted blending\n",
    "    alpha = 0.7 if user_id in rated_posts['user_id'].unique() else 0.4\n",
    "    combined_scores = {}\n",
    "    \n",
    "    for post_id in set(content_scores.keys()).union(collab_scores.keys()):\n",
    "        content_score = content_scores.get(post_id, 0)\n",
    "        collab_score = collab_scores.get(post_id, 0)\n",
    "        combined_scores[post_id] = alpha * content_score + (1 - alpha) * collab_score\n",
    "\n",
    "    # Apply category and mood filters\n",
    "    filtered_posts = all_posts.copy()\n",
    "    \n",
    "    # Check if 'category' column exists and is nested\n",
    "    if category_id:\n",
    "        try:\n",
    "            # Assuming category is a nested dictionary\n",
    "            filtered_posts = filtered_posts[filtered_posts['category'].apply(lambda x: x.get('id') == int(category_id))]\n",
    "        except (KeyError, TypeError) as e:\n",
    "            print(f\"Warning: Could not filter by category. Error: {e}\")\n",
    "    \n",
    "    # Add robust mood filtering\n",
    "    if mood:\n",
    "        try:\n",
    "            # Check different possible ways mood might be stored\n",
    "            if 'mood' in filtered_posts.columns:\n",
    "                filtered_posts = filtered_posts[filtered_posts['mood'] == mood]\n",
    "            elif 'emotions' in filtered_posts.columns:\n",
    "                # If emotions is a list or nested structure\n",
    "                filtered_posts = filtered_posts[filtered_posts['emotions'].apply(lambda x: mood in x)]\n",
    "            else:\n",
    "                print(f\"Warning: No mood column found. Cannot filter by mood: {mood}\")\n",
    "        except (KeyError, TypeError) as e:\n",
    "            print(f\"Warning: Could not filter by mood. Error: {e}\")\n",
    "\n",
    "    # Rank and select top recommendations\n",
    "    ranked_posts = sorted(\n",
    "        ((post_id, combined_scores.get(post_id, 0)) for post_id in filtered_posts['post_id']),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    recommended_post_ids = [post_id for post_id, _ in ranked_posts[:num_recommendations]]\n",
    "\n",
    "    return recommended_post_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[325, 1029, 200, 1226, 82, 665, 921, 155, 1246, 1059]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_hybrid(user_id=1, category_id=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
